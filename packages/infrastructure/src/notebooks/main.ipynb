{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a8ff31",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "%pip install -U opensearch-py\n",
    "%pip install -U boto3\n",
    "%pip install -U retrying\n",
    "%pip install -U jq\n",
    "%pip install -U langchain\n",
    "from IPython.core.display import HTML\n",
    "import warnings\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"Restarted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa76325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import boto3\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "import time\n",
    "import json\n",
    "import jq\n",
    "from aws import Aws\n",
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "from langchain.embeddings.bedrock import BedrockEmbeddings\n",
    "from langchain_community.chat_models import BedrockChat\n",
    "from langchain_core.documents.base import Document\n",
    "import pandas\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b6507348988bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"Main\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.propagate = False\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "logger.info(\"Logger initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724823fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "aoss_node=\"mey2dta082iatb0w5x43.us-east-1.aoss.amazonaws.com\"\n",
    "index_name = \"snowflake\"\n",
    "database_name=\"snowflake\"\n",
    "catalog_name=\"snowflake\"\n",
    "service=\"aoss\"\n",
    "glue_databucket_name=\"text-to-sql-with-athena-and-sn-assetbucket1d025086-uvkbiuwwsgqb\"\n",
    "query_result_folder=\"athena-workgroup\"\n",
    "athena_work_group=\"snowflake-workgroup\"\n",
    "aws=Aws()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e850de01db86304",
   "metadata": {},
   "outputs": [],
   "source": [
    "sts_client = boto3.client('sts')\n",
    "response = aws.sts.get_caller_identity()\n",
    "logger.info(json.dumps(response))\n",
    "boto3_session = boto3.session.Session()\n",
    "region = boto3_session.region_name\n",
    "account_id=aws.sts.get_caller_identity().get('Account')\n",
    "credentials = boto3.Session().get_credentials()\n",
    "auth = AWSV4SignerAuth(credentials, region, service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dbec4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "response = aws.glue.get_tables(\n",
    "    CatalogId=account_id,\n",
    "    DatabaseName=database_name,\n",
    ")\n",
    "\n",
    "program=jq.compile(\".TableList[] |  {Catalog: .DatabaseName, Database: (.StorageDescriptor.Location | split(\\\".\\\") | .[1]), Table: (.StorageDescriptor.Location | split(\\\".\\\") | .[2]), Columns: (.StorageDescriptor.Columns | map({Name:.Name , Type: .Type, Comment: .Comment})) }\")\n",
    "snowflake_tables=program.input(json.loads(json.dumps(response, indent=4, sort_keys=True, default=str))).all()\n",
    "print(snowflake_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28762e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=[]\n",
    "for table in snowflake_tables:\n",
    "    docs.append(Document(json.dumps(table)))\n",
    "logger.info(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c204af91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embeddings = BedrockEmbeddings(\n",
    "            client=aws.bedrock, \n",
    "            model_id=\"amazon.titan-embed-text-v1\"\n",
    "        )\n",
    "# Build the OpenSearch client\n",
    "oss_client = OpenSearch(\n",
    "    hosts=[{'host': aoss_node, 'port': 443}],\n",
    "    http_auth=auth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection,\n",
    "    timeout=300\n",
    ")\n",
    "if oss_client.indices.exists(index=index_name):\n",
    "    logger.info(f\"Dropping existing index '{index_name}'\")\n",
    "    response = oss_client.indices.delete(\n",
    "        index = index_name\n",
    "    )\n",
    "    logger.info(json.dumps(response))\n",
    "    time.sleep(10)\n",
    "    \n",
    "# Create index\n",
    "logger.info(f\"Creating index '{index_name}'\")\n",
    "vector_search=OpenSearchVectorSearch(opensearch_url=f\"https://{aoss_node}\", \n",
    "                                     index_name=index_name,\n",
    "                                     embedding_function=embeddings,\n",
    "                                     http_auth=auth,\n",
    "                                    engine=\"faiss\")\n",
    "time.sleep(10)\n",
    "vector_search.client=oss_client\n",
    "logger.info(f\"Indexing documents...\")\n",
    "response=vector_search.add_documents(documents=docs)\n",
    "time.sleep(60) #it can take up to a minute for the documents to finish indexing\n",
    "logger.info(response)\n",
    "logger.info(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01647a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_modifier = {\n",
    "                \"max_tokens\": 3000,\n",
    "                \"temperature\": 0,\n",
    "                \"top_k\": 20,\n",
    "                \"top_p\": 1,\n",
    "                \"stop_sequences\": [\"\\n\\nHuman:\"],\n",
    "            }\n",
    "llm = BedrockChat(model_id = \"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "                            client = aws.bedrock, \n",
    "                            model_kwargs = inference_modifier \n",
    "                            ) \n",
    "\n",
    "\n",
    "def wait_for_result(execution_id):\n",
    "    logger.info(f\"Getting status of query: {execution_id}\")\n",
    "   \n",
    "    results = aws.athena.get_query_execution(QueryExecutionId=execution_id)\n",
    "    #logger.info(f\"Results: {results}\")\n",
    "    status=results['QueryExecution']['Status']['State']\n",
    "    logger.info(f\"Status {status}\")\n",
    "    if status in ['SUCCEEDED','FAILED','CANCELLED']:\n",
    "        return results\n",
    "    else:\n",
    "        time.sleep(5) \n",
    "        return wait_for_result(execution_id)\n",
    "\n",
    "def syntax_checker(query_string):\n",
    "        # print(\"Inside execute query\", query_string)\n",
    "        query_config = {\n",
    "            \"OutputLocation\": f\"s3://{glue_databucket_name}/{query_result_folder}\",\n",
    "            \"EncryptionConfiguration\": {\n",
    "                'EncryptionOption': 'SSE_S3',\n",
    "\n",
    "            }\n",
    "        }\n",
    "        query_execution_context = {\n",
    "            \"Catalog\": catalog_name,\n",
    "        }\n",
    "        query_string=\"Explain  \"+query_string\n",
    "        logger.info(f\"Executing: {query_string}\")\n",
    "        try:\n",
    "            logger.info(\"I am checking the syntax\")\n",
    "            query_execution = aws.athena.start_query_execution(\n",
    "                QueryString=query_string,\n",
    "                ResultConfiguration=query_config,\n",
    "                QueryExecutionContext=query_execution_context,\n",
    "                WorkGroup=athena_work_group\n",
    "            )\n",
    "            execution_id = query_execution[\"QueryExecutionId\"]\n",
    "            \n",
    "            results = wait_for_result(execution_id)\n",
    "            # print(f\"results: {results}\")\n",
    "            status=results['QueryExecution']['Status']\n",
    "            logger.info(\"Status: \",status)\n",
    "            if status['State']=='SUCCEEDED':\n",
    "                return \"Passed\"\n",
    "            else:  \n",
    "                print(results['QueryExecution']['Status']['StateChangeReason'])\n",
    "                errmsg=results['QueryExecution']['Status']['StateChangeReason']\n",
    "                return errmsg\n",
    "            # return results\n",
    "        except Exception as e:\n",
    "            print(\"Error in exception\")\n",
    "            msg = str(e)\n",
    "            print(msg)\n",
    "\n",
    "def get_results(query_string):\n",
    "    query_config = {\n",
    "        \"OutputLocation\": f\"s3://{glue_databucket_name}/{query_result_folder}\",\n",
    "        \"EncryptionConfiguration\": {\n",
    "            'EncryptionOption': 'SSE_S3',\n",
    "            \n",
    "        }\n",
    "    }\n",
    "    query_execution_context = {\n",
    "        \"Catalog\": catalog_name,\n",
    "    }\n",
    "    query_execution = aws.athena.start_query_execution(\n",
    "        QueryString=query_string,\n",
    "        ResultConfiguration=query_config,\n",
    "        QueryExecutionContext=query_execution_context,\n",
    "        WorkGroup=athena_work_group\n",
    "    )\n",
    "    execution_id = query_execution[\"QueryExecutionId\"]\n",
    "    results = wait_for_result(execution_id)\n",
    "    return results\n",
    "\n",
    "def generate_sql(prompt, max_attempt=4) ->str:\n",
    "            \"\"\"\n",
    "            Generate and Validate SQL query.\n",
    "\n",
    "            Args:\n",
    "            - prompt (str): Prompt is user input and metadata from Rag to generating SQL.\n",
    "            - max_attempt (int): Maximum number of attempts correct the syntax SQL.\n",
    "\n",
    "            Returns:\n",
    "            - string: Sql query is returned .\n",
    "            \"\"\"\n",
    "            attempt = 0\n",
    "            error_messages = []\n",
    "            prompts = [prompt]\n",
    "            sql_query=\"\"   \n",
    "            while attempt < max_attempt:\n",
    "                logger.info(f'Sql Generation attempt Count: {attempt+1}')\n",
    "                try:\n",
    "                    logger.info(f'Attempt #{attempt+1} to generate the sql')\n",
    "                    generated_sql = llm.invoke(prompt)\n",
    "                    logger.info(generated_sql)\n",
    "                    query_str = generated_sql.content.split(\"```\")[1]\n",
    "                    query_str = \" \".join(query_str.split(\"\\n\")).strip()\n",
    "                    logger.info(query_str)\n",
    "                    sql_query = query_str[3:] if query_str.startswith(\"sql\") else query_str\n",
    "                    # return sql_query\n",
    "                    syntaxcheckmsg=syntax_checker(sql_query)\n",
    "                    if syntaxcheckmsg=='Passed':\n",
    "                        logger.info(f'syntax checked for query passed in attempt number :{attempt+1}')\n",
    "                        return sql_query\n",
    "                    else:\n",
    "                        prompt = f\"\"\"{prompt}\n",
    "                        This is syntax error: {syntaxcheckmsg}. \n",
    "                        To correct this, please generate an alternative SQL query which will correct the syntax error.\n",
    "                        The updated query should take care of all the syntax issues encountered.\n",
    "                        Follow the instructions mentioned above to remediate the error. \n",
    "                        Update the below SQL query to resolve the issue:\n",
    "                        {sql_query}\n",
    "                        Make sure the updated SQL query aligns with the requirements provided in the initial question.\"\"\"\n",
    "                        prompts.append(prompt)\n",
    "                        attempt += 1\n",
    "                except Exception as e:\n",
    "                    logger.error(f'FAILED: {e}')\n",
    "                    msg = str(e)\n",
    "                    error_messages.append(msg)\n",
    "                    attempt += 1\n",
    "            return sql_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365c6fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "user_query='show me all non-adult movie titles from 1980'\n",
    "logger.info(f'Searching metadata from vector store')\n",
    "\n",
    "# vector_search_match=rqst.getEmbeddding(user_query)\n",
    "vector_search_match = vector_search.similarity_search(user_query)\n",
    "logger.info(vector_search_match)\n",
    "\n",
    "if len(vector_search_match)>0: \n",
    "    page_contents=[]\n",
    "    for x in vector_search_match:\n",
    "        page_contents.append(x.page_content)\n",
    "    \n",
    "    details=\"It is important that the SQL query complies with Athena syntax. During join if column name are same please use alias ex llm.customer_id in select statement. It is also important to respect the type of columns: if a column is string, the value should be enclosed in quotes. If you are writing CTEs then include all the required columns. Please print the resulting SQL query in a sql code markdown block.\"\n",
    "    final_question = \"\\n\\nHuman:\"+details +\". The following json document represents the metadata for the tables in the database:  \"+ \", \".join(page_contents)+\". Generate SQL to select \"+ user_query+ \"\\n\\nAssistant:\"\n",
    "    logger.info(final_question)\n",
    "    query_string = generate_sql(final_question)\n",
    "    results=get_results(query_string)\n",
    "    if results['QueryExecution']['Status']['State']==\"FAILED\":\n",
    "        logger.error(results)\n",
    "    output_location=results['QueryExecution']['ResultConfiguration']['OutputLocation']\n",
    "else:\n",
    "    logger.error(\"No vector search match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9be147c",
   "metadata": {},
   "outputs": [],
   "source": [
    "url=urlparse(output_location)\n",
    "obj=aws.s3.get_object(Bucket=url.netloc, Key=url.path.lstrip('/')) \n",
    "\n",
    "results_df = pandas.read_csv(obj['Body'])\n",
    "display(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
